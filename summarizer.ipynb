{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZy9aQpgSpYu55obAQsWUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Souravdey15/Google-news-Webscraper/blob/master/summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L589IQ1jHvC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import datetime\n",
        "import spacy\n",
        "spacy.load('en_core_web_sm')\n",
        "import heapq\n",
        "import json\n",
        "import os.path\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from nltk import ne_chunk\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy import displacy\n",
        "from wordcloud import WordCloud\n",
        "import os\n",
        "from os import path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uXUe8GqH6oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if(path.exists(\"summaries\")!=True):\n",
        "    os.mkdir(\"summaries\")\n",
        "\n",
        "stop_words = set(\n",
        "        stopwords.words('english') +\n",
        "        ['i', 'he', 'me', 'she', 'it', 'them', 'her', 'him'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByeCVf1hH_7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punc(sent):\n",
        "    #\n",
        "    punctuations = '''!()-[]{};'\"\\,<>/?@#%^&*_~'''\n",
        "    for x in sent:\n",
        "        if x in punctuations:\n",
        "            sent = sent.replace(x, \" \")\n",
        "    return sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-XgYus1IDh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocess(sent):\n",
        "    #\n",
        "    sent = remove_punc(sent)\n",
        "    sent = nltk.word_tokenize(sent, language='english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    sent = [lemmatizer.lemmatize(x) for x in sent]\n",
        "    sent = ' '.join(sent)\n",
        "    filtered_sentence = [\n",
        "        w for w in sent.split(' ') if not w.lower() in stop_words\n",
        "    ]\n",
        "\n",
        "    return ' '.join(filtered_sentence).lower()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1APjQKaTIGk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_freq(sent):\n",
        "    word_frequencies = {}\n",
        "    for word in sent:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequncy)\n",
        "\n",
        "    return word_frequencies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__AjpegbIIy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_score_calc(text, word_frequencies):\n",
        "    sentence_list = nltk.sent_tokenize(text)\n",
        "    sentence_scores = {}\n",
        "    for sent in sentence_list:\n",
        "        for word in nltk.word_tokenize(sent.lower()):\n",
        "            if word in word_frequencies.keys():\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]\n",
        "    return sentence_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThpsycVILzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractive_summary(f, docu):\n",
        "    max_freq = weighted_freq(docu)\n",
        "    sent_scores = sent_score_calc(f, max_freq)\n",
        "    no_of_lines = len(docu.split('.'))\n",
        "    summary_sentences = heapq.nlargest(int(no_of_lines / 3),\n",
        "                                       sent_scores,\n",
        "                                       key=sent_scores.get)\n",
        "    #summary_sentences =sorted(sent_scores, key=sent_scores.get, reverse=True)[:int(no_of_lines/2)]  SUMMARY OF EACH LINE CAN BE GENERATED\n",
        "\n",
        "    summary = ' '.join(i.capitalize() for i in summary_sentences)\n",
        "    print(summary)\n",
        "    fileName = docu\n",
        "    try:\n",
        "        with open(\"summaries/\"+fileName, mode='x') as f:\n",
        "            f.write(summary)\n",
        "    except:\n",
        "        print(f+\" already created\")\n",
        "        pass\n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-dAXSH7IZa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def return_context(docu):\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(docu)\n",
        "    fin_dic = {}\n",
        "    for ent in doc.ents:\n",
        "        fin_dic[ent.text] = ent.label_\n",
        "    return json.dumps(fin_dic, sort_keys=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRjBR30vIcvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for fileName in os.listdir(\"transcriptions/\"):\n",
        "\n",
        "    f = open(fileName, 'r').read().lower()\n",
        "\n",
        "    f = re.sub(r'\\s+', ' ', f)\n",
        "    no_of_lines = len(open('convotext.txt', 'r').readlines())\n",
        "    \n",
        "\n",
        "    \" \".join(f.split(\"\\n\"))\n",
        "\n",
        "    extractive_summary(\" \".join(f.split(\"\\n\")), docu=f)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}